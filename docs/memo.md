- **ユーザモードのプロセスの処理から、システムコールを介してカーネルの処理を呼び出す**
	- 各種プロセスは、プロセス生成、ハードウェアの操作など、カーネルの助けが必要な時はシステムコールという手段でカーネルに処理を依頼

- システムコールの種類
	- プロセス生成、削除
	- メモリ確保、解放
	- プロセス間通信
	- ネットワーク
	- ファイルシステム操作
	- デバイスアクセス

- システムコールはC言語などの高級言語から直接呼び出せないため、アーキテクチャ依存のアセンブリコードを使って呼び出す必要がある

- **OSの助けがなければ、各プログラムはシステムコールを発行する度に、アーキテクチャ依存のアセンブリソースを書いてそれを高級言語から呼び出さないといけない**
	- **OSはシステムコールを呼び出すだけのラッパーと呼ばれる関数を提供**
		- glibc(GNUプロジェクトが提供する libc) はシステムコールのラッパー関数を含む
		- プログラムがどのようなライブラリをリンクしているかは ldd コマンドで確かめられる
		```shell
		# python3 も内部的には標準Cライブラリを使っている
		$ ldd /usr/bin/python3
		linux-vdso.so.1 =>  (0x00007ffe8d9ed000)
		libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fdb7e0cc000)
		libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fdb7dd02000)
		libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fdb7dafe000)
		libutil.so.1 => /lib/x86_64-linux-gnu/libutil.so.1 (0x00007fdb7d8fb000)
		libexpat.so.1 => /lib/x86_64-linux-gnu/libexpat.so.1 (0x00007fdb7d6d2000)
		libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007fdb7d4b8000)
		libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fdb7d1af000)
		/lib64/ld-linux-x86-64.so.2 (0x00007fdb7e2e9000)		
		```

	- **高級言語で書かれたユーザプログラムからは、各言語に対して用意されているシステムコールのラッパー関数を呼び出すだけで済む**

- カーネルに処理を依頼するための**システムコールを発行すると、CPUにおいて割り込みが発生**する
	- ユーザモードからカーネルモードに遷移して、依頼に応じたカーネルの処理が動き始める
	- カーネル内のシステムコール処理が終了すれば、ユーザモードに戻ってプロセスの動作を継続する
	- 不正な要求(システムに存在しない量のメモリを要求など)であれば、システムコールが失敗する

- プロセスがユーザモードとカーネルモードのどちらで実行しているかの割合は sar コマンドから得られる
	- **CPU コアがカーネルモードでシステムコールなどの処理を実行している時間の割合は %system によって得られる**
		- %system の割合が大きくなっている際は、むやみにシステムコールを発行しすぎたりシステムの負荷が高すぎる状態
	- **ユーザモードでプロセスを実行している時間の割合は %user + %nice の合計で得られる**

```shell
$ sar -P ALL 1 1
# 2コアの場合
Linux 4.4.0-210-generic (linux-hands-on)        08/06/2021      _x86_64_        (2 CPU)

12:49:55 AM     CPU     %user     %nice   %system   %iowait    %steal     %idle
12:49:56 AM     all      0.00      0.00      0.00      0.00      0.00    100.00
12:49:56 AM       0      0.00      0.00      0.00      0.00      0.00    100.00
12:49:56 AM       1      0.00      0.00      0.00      0.00      0.00    100.00

Average:        CPU     %user     %nice   %system   %iowait    %steal     %idle
Average:        all      0.00      0.00      0.00      0.00      0.00    100.00
Average:          0      0.00      0.00      0.00      0.00      0.00    100.00
Average:          1      0.00      0.00      0.00      0.00      0.00    100.00
```

- ループさせるだけのプログラムを実行
	- すべてユーザモードで動作(loopプログラムが動作)

```shell
$ ./loop &

$ sar -P 1 1 1
Linux 4.4.0-210-generic (linux-hands-on)        08/06/2021      _x86_64_        (2 CPU)

12:54:01 AM     CPU     %user     %nice   %system   %iowait    %steal     %idle
12:54:02 AM       1    100.00      0.00      0.00      0.00      0.00      0.00
Average:          1    100.00      0.00      0.00      0.00      0.00      0.00
```

---
- プロセスの生成の目的は2種類
	- 同じプログラムの処理を複数のプロセスに分けて処理する(Webサーバによる複数リクエストの受付)
		- fork() 関数のみを使用
	- 別のプログラムを生成(bash から各種プログラムの新規生成)
		- execve() 関数を発行

- プロセスの情報の詳細はファイルで確認できる

```shell
# 出力されるアドレスは、仮想アドレス
$ cat /proc/1/maps
5593d44b0000-5593d4610000 r-xp 00000000 08:01 2275                       /lib/systemd/systemd
```

- カーネルは複数プロセスを同時に動作させるように見せるために「プロセススケジューラー」という機能を持っている
	- 1つのCPU上で同時に処理するプロセスは1つだけ
	- 複数プロセスが実行可能な場合、個々のプロセスを適当な長さの時間ごとに(タイムスライス)CUP上で順番に実行

- プロセスはロードバランサー機能によって、システムの負荷に応じて複数の論理CPUをまたいで実行することがある
	- 観測の精度を高めるためには、OS が提供する taskset コマンドで指定した論理CPU上で実行させる
		- プロセスの実行を特定の論理CPUに制限する sched_setaffinity() というシステムコールを呼び出している
	```shell
	# 0番目の論理CPUのみで実行
	taskset -c 0 ./sched 1 100 1
	```

- 2つのプロセスは交互に論理CPUを使う(同時に1つの論理CPUを使えない)

- 何個のプロセスを実行していようが、ある瞬間に1論理CPU上で動作できるプロセスは1つだけ
	- 論理CPU上では、複数のプロセスがプロセスを順番に1つずつ動かして、1周したら最初のプロセスを動かすラウンドロビン方式で動作
	- 論理CPU上でプロセスが切り替わることをコンテキストスイッチと呼ぶ
		- タイムスライスが切れると容赦なくコンテキストスイッチは発生する
		```
		# foo() 実行直後にタイムスライスが切れた場合、bar()の実行はしばらく後になる
		void main(vlid)
		{
			foo()
			bar()
		}
		```
		- **ある処理の完了までに想定より時間がかかった場合、処理自体に問題があると断定せず、コンテキストスイッチが発生して他のプロセスが動いた可能性も考慮する**

- プロセスの主な状態
	- 実行状態
		- 現在論理CPUを使っている
	- 実行待ち状態
		- CPU時間が割り当てられるのを待っている
	- スリープ状態(ほとんどはこれ)
		- 何らかしらのイベントが発生するのを待っていてそれまではCPU時間は使わない
			- 所定時間経過するのを待つ
			- ユーザ入力を待つ
			- ストレージデバイスへの読み書きの終了を待つ
			- ネットワークによるデータ送受信の終了を待つ
	- ゾンビ状態
		- 終了後に親プロセスが終了状態を受け取るのを待っている

- プロセスは生成されたらCPU時間を使い切って終わり、ではなく複数の状態に何度も遷移する

```shell
# STAT フィールドの文字を見れば状態はわかる
# R: 実行状態 or 実行待ち状態
# S: スリープ状態。シグナルによって実行状態に戻るもの
# D: スリープ状態。ストレージデバイスのアクセス待ち。長時間この状態の場合はストレージのI/Oが終了しない状態になっている可能性あり
# Z: ゾンビ状態
$ ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  37536  5516 ?        Ss   02:09   0:01 /sbin/init
root         2  0.0  0.0      0     0 ?        S    02:09   0:00 [kthreadd]
root         3  0.0  0.0      0     0 ?        S    02:09   0:00 [ksoftirqd/0]
vagrant   1855  0.0  0.1  92796  3312 ?        R    02:18   0:00 sshd: vagrant@pts/0
root      2087  0.0  0.1  36080  3192 pts/0    R+   05:54   0:00 ps aux

# bash が S なのはユーザから入力を待っているから
$ ps aux | grep bash | grep -v grep
vagrant   1856  0.0  0.2  21304  5276 pts/0    Ss   02:18   0:00 -bash
```

- 1つのプロセスが1論理CPUを専有していて、それがスリープした場合
	- 論理CPU上ではプロセスが動いていない時は、アイドルプロセスという特殊なプロセスが動作
	- 論理CPUを休止状態にし、1つ以上のプロセスが実行可能状態になるまで消費電力を押さえた状態で待機

- ***特に動いているプロセスがない場合は、論理CPUは idle 状態***

```shell
$ ps auxwww | awk '{if($8 == "R+"){print $0}}'
root      2492  0.0  0.1  36080  3288 pts/0    R+   06:20   0:00 ps auxwww

$ sar -P 1 1 1
Linux 4.4.0-210-generic (linux-hands-on)        08/06/2021      _x86_64_        (2 CPU)

06:16:11 AM     CPU     %user     %nice   %system   %iowait    %steal     %idle
06:16:12 AM       1      0.00      0.00      0.00      0.00      0.00    100.00
Average:          1      0.00      0.00      0.00      0.00      0.00    100.00
```

- プロセスを動かし続けると idle は0

```shell
$ taskset -c 0 python3 ./loop.py &
[1] 2513

06:23:31 AM     CPU     %user     %nice   %system   %iowait    %steal     %idle
06:23:32 AM       0    100.00      0.00      0.00      0.00      0.00      0.00
Average:          0    100.00      0.00      0.00      0.00      0.00      0.00
```

---
- スループット
	- 単位時間あたりの総仕事量、高いほどよい(論理CPUの計算リソースを使い切っている、 idle が短い)
- レイテンシ
	- 処理の開始から終了までの経過時間、短いほどよい

- 論理CPUがアイドル状態にならない状況では、いくらプロセスを増やしてもスループットは変わらない
	- %idle が0%でプロセスを増やし続けるとコンテキストスイッチのオーバヘッド増加によってスループットは下がる

- プロセスを増やすほど、レイテンシは悪化する

- 論理CPUが常に動いているかつ、実行待ちのプロセスがない場合に、スループットもレイテンシも最も良くなる
	- 実際のシステムにおいては、スループットとレイテンシはトレードオフの関係になることが多い
		- プロセスが動作中に次のプロセスが実行可能状態になると、スループットは高くなるが、レイテンシが長くなる

```shell
$ jobs -l
[1]  16028 Running                 taskset -c 0 python3 ./loop.py &
[2]  16029 Running                 taskset -c 0 python3 ./loop.py &
[3]- 16030 Running                 taskset -c 0 python3 ./loop.py &
[4]+ 16031 Running                 taskset -c 0 python3 ./loop.py &

# runq-sq フィールドは実行中および実行待ちのプロセスを示す
$ sar -q 1 1
Linux 4.4.0-210-generic (linux-hands-on)        08/06/2021      _x86_64_        (2 CPU)

06:59:27 AM   runq-sz  plist-sz   ldavg-1   ldavg-5  ldavg-15   blocked
06:59:28 AM         4       136      1.01      0.23      0.14         0
Average:            4       136      1.01      0.23      0.14         0
Average:            0       132      0.00      0.00      0.07         0
```

- 論理CPUが複数ある場合は、グローバルスケジューラーが、複数の論理CPU間でプロセスを公平に分配する

```shell
# 論理CPUの数の確認
$ grep -c processor /proc/cpuinfo 
2
```

- **1つのCPU上で同時に処理するプロセスは1つだけ**
	- 複数プロセスが実行可能な場合、個々のプロセスをタイムスライスごとに順番に処理する

- マルチコアCPU環境では、複数のプロセスを同時に動かさないとスループットは上がらない

- time コマンドで

```shell
$ time taskset -c 0 ./sched 1 10000 10000
estimating the workload which takes just one milli-second...
end estimation
0       9326    100

# real は経過時間
# user, sys を足したものが「実行状態」であった使用時間
# 使用時間に「スリープ状態」、「実行待ち状態」の時間を足すと、経過時間になる
# user はユーザモードでCPU時間を使っていた時間
# sys はユーザモードの処理から依頼されてカーネルがシステムコールを実行していた時間
real    0m10.622s
user    0m10.612s 
sys     0m0.004s

# プログラム終了までに10秒経過するが、論理CPUをほとんど使っていないため、
# 使用時間はほぼ0になる = ずっとプロセスがスリープ状態のため、論理CPUはアイドル状態
$ time sleep 10

real    0m10.001s
user    0m0.000s
sys     0m0.000s
```

- 特定のプロセスに実行優先度をつけるためのシステムコールが nice()
	- -19 が最も優先度が高く、通常用多くのCPU時間を得られる

---
- メモリに関する情報の確認

```shell
$ free -h
              total        used        free      shared  buff/cache   available
Mem:           2.0G         47M        1.5G        3.1M        456M        1.7G
Swap:            0B          0B          0B

$ sar -r 1 1
Linux 4.4.0-210-generic (linux-hands-on)        08/07/2021      _x86_64_        (2 CPU)

10:17:28 AM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty
10:17:29 AM   1532264    515772     25.18     27788    378624    251028     12.26    303260    128324        52
Average:      1532264    515772     25.18     27788    378624    251028     12.26    303260    128324        52
```

- メモリが不足すると OOM killer が発動し、適当なプロセスを選んで kill する
	- サーバにおいては、 sysctl の `vm.panic_on_oom` パラメータを変更して OOM killer 発動ではなく、システムを強制終了させる設定をするのが一般的

- カーネルがプロセスにメモリを割り当てるケースは、プロセス生成時と生成後に追加で動的にメモリを割り当てる時

- メモリ管理に置いて、**仮想記憶がない場合** の問題点
	- (カーネルはメモリ割り当て要求が来たら、必要なサイズを空きメモリ領域から切り出して、先頭アドレスを返すとした場合)
	- **メモリの断片化**
		- メモリの獲得、解放を繰り返すとバラバラの位置にメモリが残るため大きな領域の確保に失敗する
	- **別用途のメモリにアクセスできてしまう**
		- カーネルや他のプロセスが使用しているアドレスを指定しさえすれば、それらの領域にアクセスできてしまう
		- **データの漏洩、破壊のリスク**が有る
			- カーネルのデータを破壊すると、システム全体が正しく動作しなくなる
	- マルチプロセスの扱いが困難
		- 同じプログラムをもう1つ起動し、メモリにマップしようとするとコードとデータの配置場所がオリジナルと違うため正しく動作しない
			- 無理やり別のアドレスにマップして動作を開始するにもアドレスが違うと正しく動作しない + 他のプロセスやカーネルの領域を破壊する恐れがある

- **仮想記憶は、システムに搭載されているメモリにプロセスから直接アクセスするのではなく、仮想アドレスを用いて間接的にアクセスさせる方法**
	- プロセスから見えるメモリのアドレス = 仮想アドレス
	- システムに搭載されているメモリの実際のアドレス = 物理アドレス
	- アドレスによってアクセス可能な空間 = アドレス空間

- プロセスを確認した際に出力されるアドレスは、仮想アドレス
	- **プロセスから実際のメモリに直接アクセスする方法はない**

```shell
$ cat /proc/1/maps
5593d44b0000-5593d4610000 r-xp 00000000 08:01 2275                       /lib/systemd/systemd
```

- **仮想アドレスから物理アドレスへの変換は、カーネルが使うメモリ内に保存されている「ページテーブル」を用いる**
	- メモリをページと言う単位で区切って管理し、変換はページ単位で行われる
	- **ページテーブル内の1つのページに対応するデータは「ページテーブルエントリー」** と呼ばれ、仮想アドレスと物理アドレスの対応譲歩が入っている

- とある仮想アドレスにプロセスがアクセスすると、CPUはカーネルを介さず、自動的にページテーブルの内容を参照し、対応する物理メモリへのアクセスに変換する

- 仮想アドレス空間の大きさは固定長(1~100とした場合)で、そこを超える(101)アドレスにプロセスがアクセスると、CPU上で「ページフォルト」という割り込みが発生
	- 実行中の命令が中断され、カーネル内の「ページフォルトハンドラ」が動作
	- 通知を受けたプロセスは強制終了される
	- メモリアドレスを直接扱える言語だとプログラム自身、扱えない言語だと処理系やライブラリの問題で発生しうるエラー

- 仮想記憶の仕組みを使い、カーネルがプロセスにメモリを割り当てるまでの流れ
	- プロセス生成時
		- プログラムの実行ファイルを読み出し、補助的な情報(コード領域、データ領域のサイズ等)を読み出す
		- プログラムの実行に必要なメモリサイズを割り出し、その領域を物理メモリ上に割り当てる
		- プロセスのためのページテーブルを作って、仮想アドレス空間と物理アドレス空間をマッピング
		- 実行開始
	- 追加割当時
		- プロセスが追加のメモリ要求
		- カーネルは新規にメモリを割り当て、対応するページテーブルを作成(ページテーブルが拡張される)
		- 仮想アドレスをプロセスに返す

- C言語の標準ライブラリにある malloc() というメモリ獲得関数はシステムコールへのラッパー関数の mmap() を呼び出す
	- 事前に glibc が mmap() システムコールでカーネルからページ単位で大きめのメモリ領域を確保してプール
	- プログラムからの malloc() 発行時に、プールされた領域から必要な量をバイト単位で取得してプログラムに返す
	- メモリの量を出すプログラムと Linux から見たプロセスの使用メモリ量に差分が発生するのはこのため
		- Linux から見た使用量は mmap() によって割り当てたすべてのメモリの総量を指すため、プログラムの malloc() で獲得したものより大きくなる
	- ソースコードから直接のメモリ管理を隠蔽する、高級な言語においても各オブジェクトの生成時に、C言語の malloc() や  mmap() 呼び出しによってメモリを確保している

	- Java の実行時のシステムコールを追ってみた([※参考](https://kazuhira-r.hatenablog.com/entry/20160703/1467555102))
		- バイトコードを JVM 内で実行するから?? malloc() は呼ばれていなかった ~~(Cは使っていない??)~~ -> Cのライブラリは使っているのでこれは間違い
	```shell
	$ strace -tt -o hello-java-main.log java Main 
	hello world

	# 子プロセスのシステムコールも記録
	$ strace -f -tt -o hello-java.log java Main 
	hello world
	```

- 仮想記憶があることで解決する問題
	- メモリの断片化
		- 物理メモリ上では断片化している領域を、プロセスの仮想アドレス空間上では大きな1つの空間として見せることができる
	- 別用途のメモリにアクセスできてしまう
		- 仮想アドレス空間とそれに応じたページテーブルはプロセスごとに作られる
		- プロセスは独立した仮想アドレス空間を持つことになり、他プロセスのメモリにはアクセスできなくなる
		- カーネルのメモリはすべての仮想アドレス空間にマップされているが、カーネルモード専用でアクセス可能
			- ユーザモードで動作するプロセスから盗み見たり破壊することは不可能
	- マルチプロセスの扱いが困難
		- それぞれ専用のアドレス空間で動作するプログラムを作れば良い

---
- ファイルマップ
	- プロセスがファイルの領域を仮想アドレス空間上にメモリマップする
		- mmap() を呼び出し、ファイルの内容をメモリに読み出して、その領域を仮想アドレス空間にマップする
		- マップしたファイルには、メモリアクセスと同じ方法でアクセスできる
	- 後ほど、所定のタイミングでストレージデバイス上のファイルに書き戻される
		- ファイルに対して write(), fprintf() を発行しなくても、メモリマップされた領域を memcpy() によってコピーするだけでファイルの内容が更新できる

- デマンドページング
	- プロセスの仮想アドレス空間内の各ページに対応する物理メモリを、当該ページに最初にアクセスしたときに割り当てる方式
		- これがないと mmap() によるメモリの割り当てに無駄が発生する
			- カーネルが必要な領域を物理メモリ上に獲得する際に、獲得してから当分使わない or プロセス終了まで使わない領域が存在するから
		- プロセス生成時に、仮想アドレス空間内の対応するページに「プロセスが領域を獲得した」という情報を記録
		- ただし、物理メモリはこの時点では割り当てない
		- プログラムがエントリポイントから実行を開始する際に、エントリポイントに対応するページ用の物理メモリを割り当てる
	- mmap() によってプロセスがメモリを確保すること = 仮想メモリを確保
	- 確保した仮想メモリにアクセスして、物理メモリに紐付けられること = 物理メモリを確保
		- CPU がページテーブルを参照し、仮想アドレスが物理アドレスと紐付いていないことを確認
		- CPU においてページフォルトが発生
		- カーネルのページフォルトハンドラが仮想アドレスに対応する物理メモリを割り当て、ページテーブルを書き換える
		- ユーザモードに戻ってプロセスが実行を継続する(**プロセスは実行中にページフォルトしていたことには気づかない**)
	- デマンドページングでは、 mmap() に成功しても、獲得済みメモリへの書き込み時点で物理メモリが不足して枯渇することもある

- メモリの枯渇
	- 仮想メモリの枯渇と物理メモリの枯渇に分けられる
	- x86 アーキテクチャでは仮想アドレス空間が 4G バイトしかなかったため仮想メモリの枯渇は頻繁に起こる
	- x86_64 アーキテクチャだと 128T バイトあるので起きない

- コピーオンライト
	- プロセスの生成に使われるシステムコール fork() も仮想記憶の仕組みを使って高速化されている
	- 親プロセスのメモリを子プロセスにすべてコピーはせず、ページテーブルだけをコピーする
	- 親も子もページテーブルエントリーの全ページの書き込み権限を無効化する
	- 読み込みだけなら、どちらのプロセスも共有された物理ページにアクセス
	- 親か子のどちらかが、ページのどこかを更新しようとすると、以下の流れで共有を解除する
		- 書き込みが許可されていないのでページフォルトが発生
		- CPU がカーネルモードに遷移し、カーネルのページフォルトハンドラが動く
		- ページフォルトハンドラーがアクセスされたページを別の場所にコピーし、書き込みしようとしたプロセスに割り当て
		- 親と子のそれぞれのプロセスで共有が解除されたページに対応するページエントリーを更新する
	- 以降は共有されたページには読み書きともに自由にアクセスできる
	- **物理メモリを fork() の発行時ではなく、その後の書き込み時にコピーするためコピーオンライト(CoW)と呼ぶ**

- スワップ
	- **物理メモリが枯渇した状況で物理メモリを獲得した際に、既存の使用中の物理メモリのうち一部をストレージデバイスに対比することで空きメモリを作り出すこと**
	- カーネルが使用中の物理メモリの一部をスワップ領域に退避することを、スワップアウトという
	- ページテーブルエントリーと別の、スワップ領域管理用の場所に記録される
	- スワップ領域に対比したデータを物理メモリに戻すことを、スワップインという
	- **ページ単位で実行するため、スワップイン、スワップアウトを合わせて、Linux ではページングという**
	- ストレージデバイスへのアクセスはメモリへのアクセスに比べ格段に遅いため、スラッシング(スワップイン、アウトが繰り返されること)が起きる際はメモリを増強するべき
	- **ストレージデバイスへのアクセスが発生するページフォルトをメジャーフォルトという**

```shell
# スワッピングが発生しているかを確認
$ sar -W 1
Linux 5.4.0-81-generic (linux-hands-on)         08/17/21        _x86_64_        (2 CPU)

05:57:40     pswpin/s pswpout/s
05:57:41         0.00      0.00
05:57:42         0.00      0.00
```

- 階層型ページテーブル
	- プロセスのページテーブルは仮想アドレス空間の全てのページについて対応する物理メモリが存在するかのデータを保持している
	- 使用する仮想メモリ量が増えるとページテーブルの使用量が増えるよう、階層型ページテーブルになっている
	- これによりページテーブルに必要なメモリ量を節約する
	- `sar -r ALL 1 1` の `kbpgtbl` フィールドがページテーブルに使用している物理メモリの量
	- まれに、プロセスの作りすぎによりメモリ不足になることも

- ヒュージページ
	- fork() 時の CoW によりメモリ割り当ては親プロセスが使用している物理メモリは使用しないが、ページテーブルは親プロセスと同じサイズのものを新規作成する
	- 下位のページテーブルが埋まっている場合は、ページテーブルの階層を1段飛ばしてヒュージページのまま使うことでページテーブルのためのメモリ削減と fork() の高速化が期待できる

---
- コンピュータの動作の流れ
	- 命令を元にメモリからレジスタにデータを読み出す
	- レジスタ上のデータを元に計算する
	- 計算結果をメモリに戻す

- 全体の処理速度は、メモリアクセスのレイテンシに律速される

- **レジスタ上の計算とメモリアクセスの所要時間を埋めるために存在するのがキャッシュメモリ**
- キャッシュメモリの処理はカーネルは関与せず、ハードウェア内で完結する
	- レジスタからキャッシュメモリにデータを書き戻す際に、キャッシュラインに変更されたことを示す印をつける(ダーディである、と呼ぶ)
	- 書き込みした時点より後の所定のタイミングでバックグラウンド処理としてメモリに書き戻される(ダーティでなくなる)
	- **書き込み処理は高速なキャッシュメモリへのアクセスだけで済む**

- 階層型キャッシュメモリ
	- キャッシュメモリがいっぱいになったら、既存のキャッシュラインを破棄する
	- 廃棄するキャッシュがダーティな場合は、同期的にメモリに書き込みした上で捨てる
		- 全てのキャッシュラインがダーティな場合は、メモリアクセスのたびにキャッシュラインが入れ替わるスラッシングが発生し性能が劣化する
	- 最近の x86_64 アーキテクチャの CPU はキャッシュメモリを階層型構造にしている

```shell
# キャッシュメモリの情報の確認
$ ls -l /sys/devices/system/cpu/cpu0/cache/index0/

$ cat /sys/devices/system/cpu/cpu0/cache/index0/level 
1

$ cat /sys/devices/system/cpu/cpu0/cache/index0/size 
32K
```

- 参照の局所性
	- **時間的局所性**
		- ある時点でアクセスされたデータは近い将来に再びアクセスされる可能性が高い
		- ループ処理の中のコード領域
	- **空間的局所性**
		- ある時点であるデータにアクセスされると、それに近い場所のデータにアクセスする可能性が高い
		- 配列要素の全走査

- ページキャッシュ
	- メモリアクセスとストレージアクセスの速度の差を埋めるためのカーネルの仕組み
	- ストレージデバイス上のファイルデータをメモリにキャッシュする
	- プロセスがファイルのデータを読み出す場合
		- カーネルのメモリ上にあるページキャッシュにコピーしてからプロセスのメモリにコピー
		- ページキャッシュ上に存在するデータを読み出すと、カーネルはページキャッシュのデータを返す
			- ストレージデバイスにアクセスするのより遥かに早い
	- **ページキャッシュは全プロセス共有の資源なので、読み出し元のプロセスは別プロセスでも良い**
	- プロセスがデータをファイルに書き込むと、カーネルはページキャッシュだけにデータを書き込む
		- ストレージデバイスにアクセスするのより遥かに早い
	- 当該データに相当するエントリーに印をつけ、ダーティーページとする
	- **ダーティーページは所定のタイミングでカーネルのバックグラウンド処理によってストレージ内のファイルに反映される**
	- **各プロセスによってアクセスされるファイル上のデータが全て、ページキャッシュ上に存在すると、ファイルアクセス速度は実質的にメモリアクセス速度に近くなる**
	- ダーティーページがシステムの強制電源断によりなくなることを許容できない場合は、 open() システムコールでファイルを開く際に、 O_SYNC フラグを設定し、 write() が発行されるごとに、ストレージデバイスに同期的に書き込む

---
- ***Linux は直接ストレージデバイスにアクセスせず、ファイルシステムを介してアクセスする**
	- ストレージデバイスの機能は、「指定したアドレスに対して所定のサイズのデータを読み書きする」こと
	- **どこにどんなデータがあるか、どこに空き領域があるかを管理する仕組みがファイルシステム**
		- ファイルシステムがないと、アプリケーションが直にストレージデバイスに上記の命令を出す必要がある
		- また、読み出すときにデータを保存したアドレスとサイズをアプリケーションが覚えていないといけない
	- **ファイルシステムはユーザにとって意味のある塊のデータを、名前、位置、サイズなどの補助的な情報を付加した上で、ファイルという単位にして管理する**
	- データ構造をあらかじめ仕様として決めておき、カーネル内のファイルシステムを扱うための処理が、仕様を元にデータを操作する
		- ユーザは各データの名前だけ覚えておけば、ストレージデバイス上の位置やサイズなどの情報を記録しておかなくても良い
	- Linux では複数のファイルシステムを扱えるが、どれを選んでもユーザからは統一したシステムコールによってアクセスできる

- ファイルシステムにはデータ(ユーザが作成した内容)とメタデータ(ファイルの名前、ストレージデバイス上の位置、サイズなどの補助的な情報)がある

```shell
$ df -hT
Filesystem     Type      Size  Used Avail Use% Mounted on
udev           devtmpfs  977M     0  977M   0% /dev
tmpfs          tmpfs     199M  984K  198M   1% /run
/dev/sda1      ext4       39G  2.6G   37G   7% /
tmpfs          tmpfs     994M     0  994M   0% /dev/shm
tmpfs          tmpfs     5.0M     0  5.0M   0% /run/lock
tmpfs          tmpfs     994M     0  994M   0% /sys/fs/cgroup
/dev/loop0     squashfs   56M   56M     0 100% /snap/core18/2128
/dev/loop1     squashfs   33M   33M     0 100% /snap/snapd/12704
/dev/loop2     squashfs   71M   71M     0 100% /snap/lxd/21029
vagrant        vboxsf    466G  123G  344G  27% /vagrant
tmpfs          tmpfs     199M     0  199M   0% /run/user/1000
```

- ファイルシステムの不整合(書き込み途中で電源断など)を防ぐための技術
	- ジャーナリング(ext4, XFS のファイルシステムで使用)
		- ファイルシステム内にユーザには認識できないジャーナル領域を用意
		- 更新に必要なアトミックな処理の一覧をジャーナル領域に書き出す(ジャーナルログ)
		- ジャーナル領域の内容に基づいてファイルシステムの内容を更新する
		- 更新完了後に、ジャーナルログを破棄する
		- **ジャーナルログの更新中に電源断になった場合は、ジャーナル領域を捨てる**
		- **実データを更新中に電源断になった場合は、ジャーナルログを最初から再生する**
	- コピーオンライト(Btrfs)
		- ext4,XFS とは違い、ファイルを更新するたびにストレージデバイスの別の場所に書き込む
		- 更新時は別の場所に書き込み、別の場所への書き込み完了後にディレクトリからのリンクを張り替える
		- 電源断には作りかけのデータを削除すれば不整合は発生しない
		- **ファイルシステムの不整合に対する対策は、最後にバックアップをとった時点に復元すること**

- キャラクタデバイス
	- 読み出しと書き込みはできるがシークはできない
	- 端末、キーボード、マウスが代表的
	- **bash 上の操作が最終的にデバイスファイルの操作に変換される**

```shell
# ls -l /dev/tty
crw-rw-rw- 1 root tty 5, 0 Sep  6 01:25 /dev/tty

# ps aux | grep bash | grep -v grep
root        1647  0.0  0.2  10160  5376 pts/0    S    01:29   0:00 -bash

### 端末デバイスに文字列を書き込み
# echo hello > /dev/pts/0
hello

# ps aux | grep bash | egrep -v 'grep|vagrant'
root        6062  0.0  0.2  10028  4988 pts/1    S+   02:52   0:00 -bash
root        6506  0.0  0.2  10028  5100 pts/0    S    02:53   0:00 -bash

### 別端末に書き込むとその端末の標準出力に hello と表示される
# echo hello > /dev/pts/1
```

- ブロックデバイス
	- 読み書き以外に、ランダムアクセスが可能
	- HDD,SSD などのストレージデバイスが代表的なブロックデバイス
	- 以下のケース以外は、基本的にはファイルシステム経由で使用する
		- パーティションテーブルの更新
		- ブロックデバイスレベルのデータのバックアップとリストア(dd コマンド)
		- ファイルシステムの作成、マウント

```shell
# ls -l /dev/sda
brw-rw---- 1 root disk 8, 0 Sep  6 01:26 /dev/sda
```

- tmpfs はメモリベースのファイルシステム
	- ここに保存したデータは電源を切るとなくなる
	- ストレジデバイスへのアクセスが一切発生しないため、高速にアクセスできる

```
# df -hT | grep tmpfs
udev           devtmpfs  977M     0  977M   0% /dev
tmpfs          tmpfs     199M  984K  198M   1% /run
tmpfs          tmpfs     994M     0  994M   0% /dev/shm
tmpfs          tmpfs     5.0M     0  5.0M   0% /run/lock
tmpfs          tmpfs     994M     0  994M   0% /sys/fs/cgroup
tmpfs          tmpfs     199M     0  199M   0% /run/user/1000

# mount | grep ^tmpfs
tmpfs on /run type tmpfs (rw,nosuid,nodev,noexec,relatime,size=203524k,mode=755)
tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)
tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
tmpfs on /run/snapd/ns type tmpfs (rw,nosuid,nodev,noexec,relatime,size=203524k,mode=755)
tmpfs on /run/user/1000 type tmpfs (rw,nosuid,nodev,relatime,size=203520k,mode=700,uid=1000,gid=1000)
```

- 仮想ファイルシステム
	- ps, sar, top, free などのコマンドは procfs から情報を採取している

```shell
### $$ で現在のプロセスの PID が取れる
# echo $$
6062

# ps aux | grep $$ | grep -v grep
root        6062  0.0  0.2  10160  5236 pts/1    S    02:52   0:00 -bash

### proc ファイルシステム以下のファイルにアクセスすると各プロセスの情報を得られる
# ls /proc/$$
arch_status  clear_refs       cwd      gid_map    maps        net        oom_score_adj  root       smaps         status         uid_map
attr         cmdline          environ  io         mem         ns         pagemap        sched      smaps_rollup  syscall        wchan
autogroup    comm             exe      limits     mountinfo   numa_maps  patch_state    schedstat  stack         task
auxv         coredump_filter  fd       loginuid   mounts      oom_adj    personality    sessionid  stat          timers
cgroup       cpuset           fdinfo   map_files  mountstats  oom_score  projid_map     setgroups  statm         timerslack_ns

# mount | grep proc
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
```

```shell
# cat /proc/cpuinfo
# cat /proc/meminfo
```

- cgroupfs
	- 1つのプロセスや複数個からなるプロセスのグループにリソースの使用量の制限をかける cgroup は cgroupfs を介して操作


